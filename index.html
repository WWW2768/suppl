The research methodology employed a comprehensive within-subjects design involving 20 participants (10 with master's backgrounds and 10 with bachelor's degrees). Each participant was compensated with 100 RMB for approximately one hour of participation. The study meticulously controlled for variables by having each participant experience both control (standard video player) and experimental (VisAug-enhanced) conditions.
<br>
The experimental procedure consisted of three distinct phases. In the familiarization phase, participants received a thorough orientation to the system and experimental environment. During the baseline measurement phase, participants used a standard video player without any visual aids while researchers recorded task completion times and noted any difficulties encountered. In the intervention phase, participants reviewed the same video segments with VisAug's image and text enhancement features enabled at a default enhancement coefficient of 5.
<br>
Task performance analysis revealed nuanced results across different content types. For Task 1, focusing on AI's educational impact (timestamps 07:25; 19:22-21:15), the weighted accuracy increased dramatically from 77.5% to 92.5%. This improvement was attributed to VisAug's ability to highlight relevant educational concepts and maintain contextual continuity. Task 2, concerning government actions on AI technology (timestamps 13:43-14:30; 18:27-19:20), showed a modest improvement from 82.5% to 85%, suggesting that policy-related content might require different enhancement strategies. Task 3, dealing with societal concerns about AI (timestamp 2:35-3:53), demonstrated the smallest improvement from 25% to 30%, indicating challenges in visualizing abstract societal concepts.
<br>
The qualitative feedback revealed specific insights about system features. The transcript functionality received particularly strong praise from non-native speakers, with participant P2&3 noting its crucial role in content comprehension. However, users like P11&12 identified limitations in the auto-scrolling feature, suggesting the need for more user control. The keyword highlighting feature generated mixed responses - while generally appreciated for quick content scanning, participants like P7 noted that "not all highlighted keywords represented core content," suggesting the need for more sophisticated keyword selection algorithms.
<br>
Navigation challenges emerged as a significant theme, particularly for non-native language content. Users like P9 requested content segmentation based on discussion topics, while P7 suggested implementing bookmarking and annotation features. The visual enhancement component received detailed critique, with users like P13 noting that sometimes the generated images reduced the content's perceived seriousness, especially for serious topics like societal impacts.
<br>
The correlation analysis provided statistical validation of system effectiveness. The strong correlation (0.8643, p=0.0013) between understanding new information and clarifying complex concepts suggests that VisAug successfully supports cognitive processing of video content. The moderate correlation (0.7324, p=0.0160) between information understanding and engagement indicates that better comprehension leads to increased user involvement. However, the weaker correlation (0.5787, p=0.0796) between concept clarification and engagement levels suggests that simply making concepts clearer doesn't automatically ensure user engagement.
<br>
User interface preferences showed interesting patterns. Participants consistently requested a centralized video display with peripheral enhancement features, suggesting that the current split-screen design might disperse attention. They also expressed strong interest in customization options, particularly for keyword highlighting and visual enhancement styles. The subtitling feature received high praise for accuracy, but users requested more control over scrolling and display options.
<br>
Technical performance metrics revealed that VisAug's enhancement generation maintained consistent response times below 2 seconds, meeting user expectations for real-time interaction. However, some users noted occasional misalignment between generated visualizations and video content, particularly during rapid topic transitions. These insights suggest the need for more sophisticated content analysis algorithms and possibly predictive enhancement generation to maintain synchronization with video playback.
<br>
These findings provide a robust foundation for future development, highlighting both the system's strengths in content comprehension support and areas requiring refinement, particularly in navigation tools and visual enhancement relevance.
<br>